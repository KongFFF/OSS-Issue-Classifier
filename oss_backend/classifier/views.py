import os
import torch
import requests
import torch.nn.functional as F
from django.shortcuts import render
from django.http import JsonResponse
from transformers import AutoTokenizer, AutoModelForSequenceClassification

#å…¨å±€åŠ è½½ 
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, 'saved_model')
INDEX_PATH = os.path.join(BASE_DIR, 'issue_index.pt')

print(" Initializing System...")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = None
tokenizer = None
issue_index = None
issue_metadata = None

try:
    print(f"1. Loading Model from {MODEL_PATH}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    model.to(device)
    model.eval()
    
    print(f"2. Loading Index from {INDEX_PATH}...")
    if os.path.exists(INDEX_PATH):
        data = torch.load(INDEX_PATH, map_location=device)
        issue_index = data['embeddings'].to(device) # [N, 768]
        issue_metadata = data['metadata']           # List[Dict]
        print(f"   - Index size: {len(issue_metadata)} documents")
    else:
        print("âš ï¸ Warning: Index file not found. Similarity search disabled.")

    print(" System Ready!")
except Exception as e:
    print(f" Error during initialization: {e}")



def index(request):
    return render(request, 'index.html')

def predict(request):
    if request.method == 'POST':
        text = request.POST.get('text', '')
        if not text:
            return JsonResponse({'error': 'No text provided'}, status=400)
        
        if model is None:
            return JsonResponse({'error': 'Model not loaded'}, status=500)

        # 1. é¢„å¤„ç†
        inputs = tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            padding=True, 
            max_length=128
        ).to(device)

        with torch.no_grad():
            # 2. æ¨ç†ï¼šåŒæ—¶è·å– logits (åˆ†ç±») å’Œ hidden_states (å‘é‡)
            outputs = model(**inputs, output_hidden_states=True)
            
            # --- åˆ†ç±»é€»è¾‘ ---
            probs = torch.nn.functional.softmax(outputs.logits, dim=1)
            pred_idx = torch.argmax(probs).item()
            conf = probs[0][pred_idx].item()
            
            labels = {0: 'Bug ğŸ›', 1: 'Feature âœ¨'}
            pred_label = labels.get(pred_idx, 'Unknown')

            # --- ç›¸ä¼¼æ£€ç´¢é€»è¾‘ ---
            similar_issues = []
            if issue_index is not None:
                # æå–å½“å‰è¾“å…¥çš„å‘é‡ (CLS token)
                last_hidden_state = outputs.hidden_states[-1]
                query_embedding = last_hidden_state[:, 0, :]
                query_embedding = F.normalize(query_embedding, p=2, dim=1)
                
                # è®¡ç®—ç›¸ä¼¼åº¦ (Matrix Multiplication)
                # [1, 768] x [768, N] = [1, N]
                scores = torch.mm(query_embedding, issue_index.t())
                scores = scores.squeeze(0)
                
                # å– Top 3
                topk_scores, topk_indices = torch.topk(scores, k=3)
                
                for score, idx in zip(topk_scores, topk_indices):
                    meta = issue_metadata[idx.item()]
                    similar_issues.append({
                        'title': meta['title'],
                        'url': meta['url'],
                        'score': f"{score.item()*100:.1f}%"
                    })

        return JsonResponse({
            'prediction': pred_label,
            'confidence': f"{conf*100:.2f}%",
            'similar_issues': similar_issues
        })
    
    return JsonResponse({'error': 'GET not allowed'}, status=405)



#æ–°å¢ï¼šæ‰¹é‡æ‰«æé€»è¾‘

# å¡«å…¥ GitHub Token
GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN', '')
HEADERS = {'Authorization': f'token {GITHUB_TOKEN}'}

def scan_repo(request):
    """ä¸€é”®æ‰«æä»“åº“æœ€æ–° Issue (æ™ºèƒ½è¿‡æ»¤ PR ç‰ˆ)"""
    if request.method == 'POST':
        repo_name = request.POST.get('repo_name', '').strip()
        if 'github.com/' in repo_name:
            repo_name = repo_name.split('github.com/')[-1]
        
        if not repo_name:
            return JsonResponse({'error': 'Please provide a repository name'}, status=400)

        # æ ¸å¿ƒä¿®æ”¹ 1: å°† per_page è®¾ä¸º 100ï¼Œç¡®ä¿èƒ½â€œæâ€åˆ°è¢« PR æ·¹æ²¡çš„ Issue
        # å³ä½¿åªæœ‰ 12 ä¸ª Issueï¼Œè¿™æ ·ä¹Ÿèƒ½æŠŠå®ƒä»¬å…¨åŒ…è¿›æ¥
        api_url = f"https://api.github.com/repos/{repo_name}/issues?state=open&per_page=100"
        
        try:
            resp = requests.get(api_url, headers=HEADERS)
            if resp.status_code != 200:
                return JsonResponse({'error': f'GitHub API Error: {resp.status_code}'}, status=resp.status_code)
            
            raw_issues = resp.json()
        except Exception as e:
            return JsonResponse({'error': str(e)}, status=500)

        results = []
        target_count = 10  # ä¿®æ”¹ 2: å±•ç¤º 10 æ¡æœ‰æ•ˆæ•°æ®
        
        # 3. éå†ç­›é€‰
        for item in raw_issues:
            # å¦‚æœå·²ç»å‡‘å¤Ÿäº† 10 æ¡ï¼Œå°±ç›´æ¥åœæ­¢ï¼ŒèŠ‚çœæ—¶é—´
            if len(results) >= target_count:
                break

            # è·³è¿‡ PR
            if 'pull_request' in item:
                continue
                
            # æ‹¼æ¥æ–‡æœ¬
            text = item['title'] + " " + (item['body'] or "")
            
            # --- AI åˆ†æé€»è¾‘ (ä¿æŒä¸å˜) ---
            inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(device)
            with torch.no_grad():
                outputs = model(**inputs, output_hidden_states=True)
                probs = torch.nn.functional.softmax(outputs.logits, dim=1)
                pred_idx = torch.argmax(probs).item()
                conf = probs[0][pred_idx].item()
                
                # ç›¸ä¼¼åº¦æ£€æµ‹
                is_duplicate = False
                duplicate_info = ""
                if issue_index is not None:
                    last_hidden_state = outputs.hidden_states[-1]
                    query_embedding = F.normalize(last_hidden_state[:, 0, :], p=2, dim=1)
                    scores = torch.mm(query_embedding, issue_index.t()).squeeze(0)
                    best_score, best_idx = torch.topk(scores, k=1)
                    if best_score.item() > 0.85:
                        is_duplicate = True
                        meta = issue_metadata[best_idx.item()]
                        duplicate_info = f"{meta['title']} ({best_score.item()*100:.1f}%)"

            results.append({
                'number': item['number'],
                'title': item['title'],
                'url': item['html_url'],
                'type': 'Bug ğŸ›' if pred_idx == 0 else 'Feature âœ¨',
                'confidence': f"{conf*100:.0f}%",
                'is_duplicate': is_duplicate,
                'duplicate_info': duplicate_info
            })
            
        return JsonResponse({'results': results, 'repo': repo_name})

    return JsonResponse({'error': 'Method not allowed'}, status=405)
# æ£€æŸ¥ä¸€ä¸‹é€»è¾‘
